{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8564a78-d1f4-43ad-912d-feb132ead011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "user\n",
      "Write a haiku about programming\n",
      "\n",
      "Write a haiku about programming\n",
      "\n",
      "What is the purpose of programming?\n",
      "\n",
      "The purpose of programming is to make software easier to use and improve the quality of software. It helps create the software for your needs and improve the quality of the software. Programming is the process of creating instructions or instructions for a computer to follow. These instructions can be in the form of sentences, words, or even images.\n",
      "\n",
      "What programming languages are used today?\n",
      "\n",
      "Today, programming languages include C++, Java, Python, and many others. Each has its own set of features and capabilities, but they all share some common features and ideas.\n",
      "\n",
      "What is a programming language called?\n",
      "\n",
      "A programming language is called a programming language when it is designed to be easy to learn and use by humans, rather than machine. It is a simple and efficient way to create programs. Common programming languages include Python, JavaScript, and C++.\n",
      "\n",
      "How do you translate a programming language into a word-processing language?\n",
      "\n",
      "Translate a programming language into a word-processing language requires understanding the structure of the programming language, which can be confusing. The first step is to identify the keywords and keywords that appear in the programming language. For example, if the programming language is Java, the keywords that appear are:\n",
      "\n",
      "public, protected, static, enum, public interface, abstract, final\n",
      "\n",
      "The keywords in the programming language will translate into the words and symbols of a word-processing language.\n",
      "\n",
      "How do you translate a programming language into a text editor?\n",
      "\n",
      "Translate a programming language into a text editor involves selecting the desired programming language from the list and then clicking the Edit button. You can also use the keyboard shortcuts Ctrl+I, Ctrl+S, Ctrl+E, or the Insert Insert Key on your keyboard.\n",
      "\n",
      "What is a programming language called when it is designed for human-readable use?\n",
      "\n",
      "When a programming language is designed for human-readable use, it is often called a programming language. This means it can be used by people who don't understand the technical details of the programming language. For example, in the programming language Java, the keywords Java keywords are:\n",
      "\n",
      "int, float, float, String, double, Boolean, Integer\n",
      "\n",
      "What is a programming language called when it is designed for machine-readable use?\n",
      "\n",
      "When a programming language is designed for machine-readable use, it is often called a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Device setup\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-trl-lib\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.9,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc048140-c36e-416c-844e-1016fc8d1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "In programming, let a single thread weave its tale,\n",
      "A thread of code, its threads blend in,\n",
      "The world's a tapestry that wraps,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "By weaving threads of programming, a thread of thought,\n",
      "A thread of inspiration, a tale of programming's countless threads,\n",
      "The world's a tapestry that wraps,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n",
      "The threads of programming are woven,\n",
      "A thread of thought, a thread of inspiration,\n",
      "Through countless cycles, a thread of thought,\n",
      "Unraveling the mysteries of programming,\n",
      "And weaving a tapestry of inspiration,\n",
      "A tale of programming's countless threads,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-argilla\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.9,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc1145d-731c-4832-adc0-d62528c98361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "In the depths of code, a world of wonder unfolds,\n",
      "Where the lines blurred, and the secrets lay hidden,\n",
      "A symphony of programming, a symphony of code,\n",
      "A symphony of code, a symphony of wonder.\n",
      "\n",
      "This haiku captures the essence of the programming world, with its intricate dance between syntax, semantics, and creativity. The poem begins by invoking the world of programming, where lines of code are not just raw data but a symphony of meaning and emotion. The language itself is an integral part of the world, where programming languages, algorithms, and data structures come together to create a rich tapestry of meaning and purpose.\n",
      "\n",
      "The poem then delves into the world of code, where a multitude of programming languages and technologies are intertwined. From the syntax of programming languages to the semantics of data structures, the world of code is a realm of complexity, innovation, and creativity.\n",
      "\n",
      "The poem concludes by conveying the beauty of the programming world, where programming is both a language and a way of life, where creativity, elegance, and harmony intertwine to create a world of wonder. The poem invites the reader to immerse themselves in the programming world, to explore its complexities, and to discover its many facets and wonders.\n",
      "\n",
      "This haiku captures the essence of programming, inviting the reader to embark on a journey of discovery and creation, where the world of programming is woven into the fabric of life.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-argilla2\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.7,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0840a1-8e9e-49d6-b85c-22c2dbec75f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Aha! I see you're thinking about programming. What's programming? It's a way of creating and controlling computer programs. There are many different types of programming languages, but some popular ones include Java, Python, and C++.\n",
      "\n",
      "Java is a popular language for developing Java programs. It's easy to learn and has a lot of built-in features like arrays and exception handling. Python is a popular language for web development, and it's great for creating websites and developing apps. C++ is a language that's used for building large-scale programs, like video games and 3D simulations.\n",
      "\n",
      "Now, let's talk about programming languages. What's a programming language? A programming language is a set of instructions that tells a computer what to do. It's a set of commands that can be entered and executed by a computer.\n",
      "\n",
      "So, what's the difference between programming languages and artificial languages like Python? Artificial languages are languages that people create to help us think and communicate, but they don't have the same capabilities as natural languages like English or Spanish. Programming languages, on the other hand, have the ability to do things that can't be done in artificial languages, like creating complex algorithms and data structures.\n",
      "\n",
      "That's a great overview of programming languages. I hope you're now more familiar with them. I hope you find them useful and that you start exploring the world of programming. Who knows, you might even learn a new language or two!\n",
      "\n",
      "I hope you enjoy programming and learn more about it. I hope you can find a language that suits your needs and helps you express yourself more clearly. I'm sure you'll find a language that's helpful and fun to learn. Good luck with your programming journey! Welcome to our journey through time as we explore the fascinating history of the French Revolution! This period, which lasted from 1789 to 1799, marked a significant turning point in European history that changed the political, social, and economic landscape of the continent. So grab your time machines, and let's embark on this thrilling adventure!\n",
      "\n",
      "The French Revolution began when King Louis XVI of France and his advisors believed that they had reached the end of their reign. They believed that the country was too weak to govern itself, so they ordered the execution of the king and the restoration of the monarchy. The revolutionaries, known as the Jacobins, sought to create a more democratic society and overthrow the\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-argilla3\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.7,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ca380d-c9d0-46d8-a1f7-f4a955d277d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about computers and programming: a good example is:\n",
      "\n",
      "Computer, the universal language of science,\n",
      "And it will serve as a guide in the future.\n",
      "I'll use this to guide my research and development.\n",
      "(From the book, The Human Factor: How Culture Shapes Science, 2004)\n",
      "\n",
      "Your Haiku: Computer, the universal language of science,\n",
      "It will serve as a guide in the future.\n",
      "I'll use this to guide my research and development.\n",
      "\n",
      "You Have A Programming Challenge:\n",
      "In the video below, Michael Heller and Bill Hecht explain a simple way to program a computer. They say the computer's language is called C and C++. They also show the steps of a C-like programming language. You can learn more about programming languages from the Computer Science, Software Engineering, and Data Mining, Programming Languages and Environments courses.\n",
      "\n",
      "What's C-Like Language?\n",
      "C is an open-source, object-oriented, high-level programming language that allows users to program for the desktop, web, and mobile devices. It's a language that supports large-scale data analysis, data analysis software, and web application development. C-like languages like C++ and Java are commonly used for desktop applications, while Python and Ruby are used for web applications and mobile development.\n",
      "\n",
      "What's C++?\n",
      "C++ is a general-purpose programming language that supports C and C++ with advanced features like object-oriented and structured programming. C++ is a high-level language, with modern features like templates, exception handling, and object-oriented programming, that makes it a popular choice for complex software development. C++ is used in various fields, including hardware, mobile apps, and desktop applications.\n",
      "\n",
      "What's Java?\n",
      "Java is a popular programming language for JavaEE, the Java programming environment for Java applications. It's a high-level, platform-independent, object-oriented, and multi-class programming language that's easy to learn and is widely used for Java applications, including web applications, mobile apps, and enterprise apps. Java is an open-source programming language that supports cross-platform development, allowing for integration with other platforms like the Internet of Things (IoT) and mobile devices.\n",
      "\n",
      "What's Data Mining?\n",
      "Data mining is a method of discovering hidden patterns, patterns, and relationships in data using a set of rules and algorithms. Data\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-argilla_3b\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.95,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0457429-8f13-4c62-95e7-40def5d8f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "In a garden, where trees, ever-blooming, sway, a programmer's mind,\n",
      "In a world where languages, whispers, and dreams weave,\n",
      "A programmer, weaving through fields of words,\n",
      "In a code, their threads weave, a tapestry of spells.\n",
      "\n",
      "This poem captures the essence of programming: it's a process of creativity, where words come alive and their meanings weave together, making connections that spark imagination. The poem invites the reader to ponder the possibilities of programming, sparking the curiosity to explore the intricacies of programming languages, data structures, and algorithms.\n",
      "\n",
      "This haiku is a beautiful example of how a poem can evoke a powerful image in the reader's mind. It's not just about conveying a message but also about tapping into the reader's curiosity and encouraging them to think about the world of programming and its vast possibilities.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-FT-DPO-argilla1\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.9,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18287f0-e7b9-4c7a-92d8-aef46fd26f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smol",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
