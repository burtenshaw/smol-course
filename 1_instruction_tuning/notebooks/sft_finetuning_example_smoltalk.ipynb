{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "import yaml\n",
    "\n",
    "# Load the config file\n",
    "with open(\"/Users/ohnmarhtun/lab/smol-course/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the token\n",
    "hf_token = config.get(\"huggingface_hub\", {}).get(\"token\")\n",
    "\n",
    "if hf_token:\n",
    "    # Log in to Hugging Face Hub\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Access token not found in config.yaml!\")\n",
    "\n",
    "#login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 2260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 119\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# TODO: ü¶Å If your dataset is not in a format that TRL can convert to the chat template, you will need to process it. \n",
    "# Refer to the [module](../chat_templates.md)\n",
    "# Function to process dataset into TRL chat template format\n",
    "# Import necessary libraries\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Inspect the dataset structure\n",
    "print(\"Dataset Structure:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Example in Training Split:\n",
      "{'full_topic': 'Travel/Vacation destinations/Beach resorts', 'messages': [{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\", 'role': 'user'}, {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\", 'role': 'assistant'}, {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?', 'role': 'user'}, {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.', 'role': 'assistant'}, {'content': \"Okay, I'll look into those. Thanks for the recommendations!\", 'role': 'user'}, {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first training example\n",
    "print(\"First Example in Training Split:\")\n",
    "print(ds[\"train\"][0])  # Adjust 'train' if your dataset split is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Processed Dataset Entry:\n",
      "{'text': \"<|im_start|>user\\nHi there<|im_end|><|im_start|>assistant\\nHello! How can I help you today?<|im_end|><|im_start|>user\\nI'm looking for a beach resort for my next vacation. Can you recommend some popular ones?<|im_end|><|im_start|>assistant\\nSome popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.<|im_end|><|im_start|>user\\nThat sounds great. Are there any resorts in the Caribbean that are good for families?<|im_end|><|im_start|>assistant\\nYes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.<|im_end|><|im_start|>user\\nOkay, I'll look into those. Thanks for the recommendations!<|im_end|><|im_start|>assistant\\nYou're welcome. I hope you find the perfect resort for your vacation.<|im_end|>\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f342ea516e48088f9e7f64d9c32704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6383710e941f383edf17814d13685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to process dataset into TRL chat template format\n",
    "def process_dataset_to_chat_template(dataset):\n",
    "    \"\"\"\n",
    "    Processes the dataset to convert it into the TRL chat template format.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to process.\n",
    "    Returns:\n",
    "        A Dataset in the required chat template format.\n",
    "    \"\"\"\n",
    "    chat_data = []\n",
    "\n",
    "    # Iterate through each example in the dataset\n",
    "    for example in dataset:\n",
    "        # Access the 'messages' field\n",
    "        messages = example.get(\"messages\", [])\n",
    "\n",
    "        # Initialize an empty chat template\n",
    "        chat_template = \"\"\n",
    "\n",
    "        # Iterate through messages and build the conversation template\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            \n",
    "            if role and content:\n",
    "                if role == \"user\":\n",
    "                    chat_template += f\"<|im_start|>user\\n{content}<|im_end|>\"\n",
    "                elif role == \"assistant\":\n",
    "                    chat_template += f\"<|im_start|>assistant\\n{content}<|im_end|>\"\n",
    "\n",
    "        # Append the chat template if not empty\n",
    "        if chat_template:\n",
    "            chat_data.append({\"text\": chat_template})\n",
    "\n",
    "    if not chat_data:\n",
    "        raise ValueError(\"No valid examples found in the dataset! Check the dataset structure and keys.\")\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    return Dataset.from_list(chat_data)\n",
    "\n",
    "# Process the dataset\n",
    "if \"train\" in ds:\n",
    "    processed_train = process_dataset_to_chat_template(ds[\"train\"])\n",
    "else:\n",
    "    raise ValueError(\"Dataset does not have a 'train' split! Check the dataset structure.\")\n",
    "\n",
    "# Wrap the processed dataset into a DatasetDict\n",
    "processed_dataset = DatasetDict({\"train\": processed_train})\n",
    "\n",
    "if \"test\" in ds:\n",
    "    processed_test = process_dataset_to_chat_template(ds[\"test\"])\n",
    "else:\n",
    "    raise ValueError(\"Dataset does not have a 'test' split! Check the dataset structure.\")\n",
    "\n",
    "# Wrap the processed dataset into a DatasetDict\n",
    "processed_dataset = DatasetDict({\"train\": processed_train,\n",
    "                                \"test\": processed_test})\n",
    "\n",
    "# Inspect the processed dataset\n",
    "print(\"Sample Processed Dataset Entry:\")\n",
    "if len(processed_dataset[\"train\"]) > 0:\n",
    "    print(processed_dataset[\"train\"][0])\n",
    "else:\n",
    "    print(\"Processed dataset is empty!\")\n",
    "\n",
    "# Save the processed dataset\n",
    "processed_dataset.save_to_disk(\"processed_chat_template_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "dataset_path = \"processed_chat_template_dataset\"\n",
    "processed_dataset = load_from_disk(dataset_path)\n",
    "# Verify dataset splits\n",
    "train_dataset = processed_dataset[\"train\"]\n",
    "eval_dataset = processed_dataset[\"test\"]  # Ensure the dataset has a 'test' split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohnmarhtun/anaconda3/envs/smol/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/ohnmarhtun/anaconda3/envs/smol/lib/python3.11/site-packages/transformers/training_args.py:2248: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ü§ó Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "finetune_name=\"SmolLM2-135M-FT2\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:13, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.111900</td>\n",
       "      <td>1.209085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>1.174122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.108600</td>\n",
       "      <td>1.143977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.092300</td>\n",
       "      <td>1.127601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.084500</td>\n",
       "      <td>1.118386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.075500</td>\n",
       "      <td>1.108997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.046000</td>\n",
       "      <td>1.101919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.050100</td>\n",
       "      <td>1.097304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.064800</td>\n",
       "      <td>1.088775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.120800</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>1.073822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>1.077280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>1.075193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.787300</td>\n",
       "      <td>1.076182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>1.073924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.844600</td>\n",
       "      <td>1.070008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>1.070544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.859500</td>\n",
       "      <td>1.068106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>1.066864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>1.066594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model response:\n",
      "user\n",
      "Write a haiku about programming\n",
      "\n",
      "What is a haiku about? A haiku is a Japanese poetic form that uses a 5-7-5 syllable pattern. It's often used in nature and abstract poetry.\n",
      "\n",
      "What is a haiku about? A common type of haiku is the 5-7-5 pattern, which is used in nature, poetry, and many other forms. It's used to describe a flower, a leaf, or a simple word.\n",
      "\n",
      "How can I write a haiku about programming? A simple way to write a haiku about programming is to focus on the 5-7-5 pattern. Use a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and impactful haiku.\n",
      "\n",
      "What is a haiku about? A simple way to create a more memorable and engaging haiku is to focus on the 5-7-5 pattern. Try to include a word or phrase that can describe a concept, idea, or action. This can make it more meaningful and memorable for the reader.\n",
      "\n",
      "What is a haiku about? A simple way to create an engaging and memorable haiku is to focus on a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and engaging haiku.\n",
      "\n",
      "What is a haiku about? A simple way to create an engaging and memorable haiku is to focus on a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and engaging haiku.\n",
      "\n",
      "What is a haiku about? A simple way to create an engaging and memorable haiku is to focus on a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and engaging haiku.\n",
      "\n",
      "What is a haiku about? A simple way to create an engaging and memorable haiku is to focus on a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and engaging haiku.\n",
      "\n",
      "What is a haiku about? A simple way to create an engaging and memorable haiku is to focus on a simple word, phrase, or sentence that can describe a concept, idea, or action. This can help you create a more memorable and engaging\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Device setup\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./SmolLM2-135M-FT2\"  # Path where the fine-tuned model is saved\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Ensure the pad token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to the end-of-sequence token if not defined\n",
    "\n",
    "# Format the prompt with a chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Tokenize the prompt with padding and attention mask\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True,  # Ensure the input is not longer than the model's max length\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "    max_length=512,  # Adjust based on desired response length\n",
    "    temperature=0.9,  # Adjust for creativity (lower = more deterministic, higher = more creative)\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the fine-tuned model's response\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smol",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
